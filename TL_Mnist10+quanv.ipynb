{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying MNIST with a simple model and quantum embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by:  https://www.kaggle.com/code/geekysaint/solving-mnist-using-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T19:38:34.309322Z",
     "start_time": "2025-03-25T19:38:31.891222Z"
    }
   },
   "outputs": [],
   "source": [
    "# for the Boson Sampler\n",
    "import perceval as pcvl\n",
    "#import perceval.providers.scaleway as scw  # Uncomment to allow running on scaleway\n",
    "\n",
    "# for the machine learning model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from boson_sampler import BosonSampler\n",
    "from utils import MNIST_partial, accuracy, plot_training_metrics, MNIST_quantum\n",
    "from model import *\n",
    "from pqnn_model import QuantumLayer, OutputMappingStrategy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the Boson Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T19:38:38.089635Z",
     "start_time": "2025-03-25T19:38:38.086726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cpu\n"
     ]
    }
   ],
   "source": [
    "session = None\n",
    "# to run a remote session on Scaleway, uncomment the following and fill project_id and token\n",
    "# session = scw.Session(\n",
    "#                    platform=\"sim:sampling:p100\",  # or sim:sampling:h100\n",
    "#                    project_id=\"\"  # Your project id,\n",
    "#                    token=\"\"  # Your personal API key\n",
    "#                    )\n",
    "\n",
    "# start session\n",
    "if session is not None:\n",
    "    session.start()\n",
    "\n",
    "#to display it\n",
    "# pcvl.pdisplay(bs.create_circuit())\n",
    "# define device to run the model\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'DEVICE = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset : a subset of MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T19:38:39.892636Z",
     "start_time": "2025-03-25T19:38:39.738091Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset from csv file, to use for the challenge\n",
    "train_dataset = MNIST_quantum(split = 'train')\n",
    "val_dataset = MNIST_quantum(split='val')\n",
    "\n",
    "# definition of the dataloader, to process the data in the model\n",
    "# here, we need a batch size of 1 to use the boson sampler\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T19:38:40.975295Z",
     "start_time": "2025-03-25T19:38:40.953360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([10, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T19:38:42.425213Z",
     "start_time": "2025-03-25T19:38:42.420371Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_epochs, train_loader, val_loader, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    history_train_accuracy, history_val_accuracy, history_train_loss, history_val_loss = [],[],[],[]\n",
    "    for epoch in range(num_epochs):\n",
    "        # training\n",
    "        model.train()\n",
    "        train_loss_epoch = []\n",
    "        train_acc_epoch = []\n",
    "        for step, batch in enumerate(tqdm(train_loader)):\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss_epoch.append(loss.item())\n",
    "            acc = accuracy(output, labels)\n",
    "            train_acc_epoch.append(acc)\n",
    "        model.eval()\n",
    "        val_loss_epoch = []\n",
    "        val_acc_epoch = []\n",
    "        for step, batch in enumerate(tqdm(val_loader)):\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            val_loss_epoch.append(loss.item())\n",
    "            acc = accuracy(output, labels)\n",
    "            val_acc_epoch.append(acc)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {sum(train_loss_epoch)/len(train_loss_epoch):.4f}, Val Loss: {sum(val_loss_epoch)/len(val_loss_epoch):.4f}, Train Acc: {sum(train_acc_epoch)/len(train_acc_epoch):.4f}, Val Acc: {sum(val_acc_epoch)/len(val_acc_epoch):.4f}\")\n",
    "        history_train_loss.append(sum(train_loss_epoch)/len(train_loss_epoch))\n",
    "        history_train_accuracy.append(sum(train_acc_epoch)/len(train_acc_epoch))\n",
    "        history_val_loss.append(sum(val_loss_epoch)/len(val_loss_epoch))\n",
    "        history_val_accuracy.append(sum(val_acc_epoch)/len(val_acc_epoch))\n",
    "    plot_training_metrics(history_train_accuracy, history_val_accuracy, history_train_loss, history_val_loss)\n",
    "\n",
    "\n",
    "    # validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "The goal is to perform transfer learning from task A to MNIST. First, we try with task A being ImageNet. Therefore :\n",
    "* We take a ResNet18 pretrained on ImageNet\n",
    "* Classical comparison : we train the input layer and the last FC layer to classify MNIST\n",
    "* Quantum training : the FC layer is replaced by a QuantumLayer and a linear layer to map the output of the QuantumLayer to the 10 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T19:47:17.746565Z",
     "start_time": "2025-03-25T19:47:17.556896Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T18:40:43.778163Z",
     "start_time": "2025-03-25T18:40:43.770437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T19:47:20.230557Z",
     "start_time": "2025-03-25T19:47:20.225484Z"
    }
   },
   "outputs": [],
   "source": [
    "MODES = 12\n",
    "# change first conv1 to take 1 input channel as input\n",
    "pretrained_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "\n",
    "# change last FC layer to map to the 10 classes of MNIST\n",
    "pretrained_model.fc = nn.Sequential(nn.Linear(in_features=512, out_features=MODES, bias = True), nn.Linear(in_features=MODES, out_features=10, bias = True))\n",
    "# almost equivalent to LoRA weights decomposition: instead of having 512x10 weights (+ bias), we have (512*MODES)+(MODES*10) = MODES(512+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T19:49:14.063162Z",
     "start_time": "2025-03-25T19:47:23.087309Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [1:18:40<00:00,  7.87s/it]\n",
      "100%|██████████| 60/60 [07:21<00:00,  7.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Loss: 0.9570, Val Loss: 0.4393, Train Acc: 0.7110, Val Acc: 0.8883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 58/600 [07:19<1:08:22,  7.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m pretrained_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, num_epochs, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m train_loss_epoch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m train_acc_epoch \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[1;32m     10\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     11\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/utils.py:103\u001b[0m, in \u001b[0;36mMNIST_quantum.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m     img_square \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img_square)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# print(img_square.shape)\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m img_square \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquanv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_square\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m img_square \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munflatten(torch\u001b[38;5;241m.\u001b[39mtensor(img_float),\u001b[38;5;241m0\u001b[39m,(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m))\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img_square, label\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/utils.py:71\u001b[0m, in \u001b[0;36mMNIST_quantum.quanv\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# Process a squared 2x2 region of the image with a quantum circuit\u001b[39;00m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;66;03m# print(image.shape)\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m         q_results \u001b[38;5;241m=\u001b[39m \u001b[43mqlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m# Assign expectation values to different channels of the output pixel (j/2, k/2)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pqnn_model.py:204\u001b[0m, in \u001b[0;36mQuantumLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    Forward pass through the quantum layer.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Output tensor [output_size] or [batch_size, output_size]\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     quantum_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_quantum_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_mapping(quantum_output)\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pqnn_model.py:190\u001b[0m, in \u001b[0;36mQuantumLayer.get_quantum_output\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_names):\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;66;03m# Scale input parameters by 2π for full phase shifter range\u001b[39;00m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit_parameter_map[name] \u001b[38;5;241m=\u001b[39m x[:, idx] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mpi\n\u001b[0;32m--> 190\u001b[0m unitaries \u001b[38;5;241m=\u001b[39m \u001b[43mpcvl_circuit_to_pytorch_unitary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircuit_parameter_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m _, distribution \u001b[38;5;241m=\u001b[39m pytorch_slos_output_distribution(unitaries, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_map_func)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m distribution\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pcvl2torch.py:214\u001b[0m, in \u001b[0;36mpcvl_circuit_to_pytorch_unitary\u001b[0;34m(circuit, circuit_parameters)\u001b[0m\n\u001b[1;32m    211\u001b[0m cU \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mcompute_unitary(use_symbolic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Convert to PyTorch, returns a batch of torch unitaries\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m cU_torch \u001b[38;5;241m=\u001b[39m \u001b[43msympy2torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Handle components that don't span all modes\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m!=\u001b[39m circuit\u001b[38;5;241m.\u001b[39mm:\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pcvl2torch.py:130\u001b[0m, in \u001b[0;36msympy2torch\u001b[0;34m(sympy_object, map_params, batch_size)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sympy_object\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sympy_object\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m--> 130\u001b[0m             t_object[:, i, j] \u001b[38;5;241m=\u001b[39m \u001b[43msympy2torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msympy_object\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Handle symbolic parameters: return the corresponding tensor from map_params\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sympy_object, sp\u001b[38;5;241m.\u001b[39mSymbol):\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pcvl2torch.py:146\u001b[0m, in \u001b[0;36msympy2torch\u001b[0;34m(sympy_object, map_params, batch_size)\u001b[0m\n\u001b[1;32m    141\u001b[0m         t_object \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size,), \u001b[38;5;28mcomplex\u001b[39m(sympy_object), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcomplex64)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Handle operations (functions, operators) with a recursive call on the arguments\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     t_object \u001b[38;5;241m=\u001b[39m SYMPY_TO_TORCH_OPS[sympy_object\u001b[38;5;241m.\u001b[39mfunc](\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;241m*\u001b[39m[\u001b[43msympy2torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m sympy_object\u001b[38;5;241m.\u001b[39margs]\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t_object\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    149\u001b[0m         t_object \u001b[38;5;241m=\u001b[39m t_object\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(batch_size)\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pcvl2torch.py:139\u001b[0m, in \u001b[0;36msympy2torch\u001b[0;34m(sympy_object, map_params, batch_size)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sympy_object, sp\u001b[38;5;241m.\u001b[39mNumber) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sympy_object, numbers\u001b[38;5;241m.\u001b[39mNumber):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(sympy_object, sp\u001b[38;5;241m.\u001b[39mNumber) \u001b[38;5;129;01mand\u001b[39;00m sympy_object\u001b[38;5;241m.\u001b[39mis_real) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sympy_object, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[0;32m--> 139\u001b[0m         t_object \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size,), \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msympy_object\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m         t_object \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size,), \u001b[38;5;28mcomplex\u001b[39m(sympy_object), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcomplex64)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/sympy/core/numbers.py:405\u001b[0m, in \u001b[0;36mNumber.__float__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__float__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mlib\u001b[38;5;241m.\u001b[39mto_float(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_mpf_val\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m53\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/sympy/core/numbers.py:1565\u001b[0m, in \u001b[0;36mRational._as_mpf_val\u001b[0;34m(self, prec)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_mpf_val\u001b[39m(\u001b[38;5;28mself\u001b[39m, prec):\n\u001b[0;32m-> 1565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_rational\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/mpmath/libmp/libmpf.py:494\u001b[0m, in \u001b[0;36mfrom_rational\u001b[0;34m(p, q, prec, rnd)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_rational\u001b[39m(p, q, prec, rnd\u001b[38;5;241m=\u001b[39mround_fast):\n\u001b[1;32m    492\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a raw mpf from a rational number p/q, round if\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m    necessary.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmpf_div\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/mpmath/libmp/libmpf.py:974\u001b[0m, in \u001b[0;36mmpf_div\u001b[0;34m(s, t, prec, rnd)\u001b[0m\n\u001b[1;32m    972\u001b[0m sign \u001b[38;5;241m=\u001b[39m ssign \u001b[38;5;241m^\u001b[39m tsign\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tman \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnormalize1\u001b[49m\u001b[43m(\u001b[49m\u001b[43msign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msman\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msexp\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msbc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# Same strategy as for addition: if there is a remainder, perturb\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# the result a few bits outside the precision range before rounding\u001b[39;00m\n\u001b[1;32m    977\u001b[0m extra \u001b[38;5;241m=\u001b[39m prec \u001b[38;5;241m-\u001b[39m sbc \u001b[38;5;241m+\u001b[39m tbc \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m5\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# the model is fully trained\n",
    "model = pretrained_model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(model,optimizer,5,train_loader,val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T19:53:41.239321Z",
     "start_time": "2025-03-25T19:52:24.678565Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [1:16:21<00:00,  7.64s/it]\n",
      "100%|██████████| 60/60 [07:34<00:00,  7.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Loss: 0.9243, Val Loss: 0.3892, Train Acc: 0.7108, Val Acc: 0.9117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 93/600 [11:47<1:04:14,  7.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, num_epochs, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m train_loss_epoch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m train_acc_epoch \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[1;32m     10\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     11\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/utils.py:103\u001b[0m, in \u001b[0;36mMNIST_quantum.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m     img_square \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img_square)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# print(img_square.shape)\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m img_square \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquanv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_square\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m img_square \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munflatten(torch\u001b[38;5;241m.\u001b[39mtensor(img_float),\u001b[38;5;241m0\u001b[39m,(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m))\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img_square, label\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/utils.py:71\u001b[0m, in \u001b[0;36mMNIST_quantum.quanv\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# Process a squared 2x2 region of the image with a quantum circuit\u001b[39;00m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;66;03m# print(image.shape)\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m         q_results \u001b[38;5;241m=\u001b[39m \u001b[43mqlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m# Assign expectation values to different channels of the output pixel (j/2, k/2)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pqnn_model.py:204\u001b[0m, in \u001b[0;36mQuantumLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    Forward pass through the quantum layer.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Output tensor [output_size] or [batch_size, output_size]\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     quantum_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_quantum_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_mapping(quantum_output)\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pqnn_model.py:190\u001b[0m, in \u001b[0;36mQuantumLayer.get_quantum_output\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_names):\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;66;03m# Scale input parameters by 2π for full phase shifter range\u001b[39;00m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit_parameter_map[name] \u001b[38;5;241m=\u001b[39m x[:, idx] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mpi\n\u001b[0;32m--> 190\u001b[0m unitaries \u001b[38;5;241m=\u001b[39m \u001b[43mpcvl_circuit_to_pytorch_unitary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircuit_parameter_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m _, distribution \u001b[38;5;241m=\u001b[39m pytorch_slos_output_distribution(unitaries, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_map_func)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m distribution\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pcvl2torch.py:211\u001b[0m, in \u001b[0;36mpcvl_circuit_to_pytorch_unitary\u001b[0;34m(circuit, circuit_parameters)\u001b[0m\n\u001b[1;32m    206\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r, c \u001b[38;5;129;01min\u001b[39;00m circuit\u001b[38;5;241m.\u001b[39m_components:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# TODO: we should handle recursively the case where c is a circuit, otherwise sympy unitary will be too complex\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# Get component's unitary in symbolic form\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     cU \u001b[38;5;241m=\u001b[39m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_unitary\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_symbolic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# Convert to PyTorch, returns a batch of torch unitaries\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     cU_torch \u001b[38;5;241m=\u001b[39m sympy2torch(cU, map_params, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/perceval/components/linear_circuit.py:90\u001b[0m, in \u001b[0;36mACircuit.compute_unitary\u001b[0;34m(self, assign, use_symbolic, use_polarization)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m use_polarization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     use_polarization \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_unitary\u001b[49m\u001b[43m(\u001b[49m\u001b[43massign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_symbolic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_polarization \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_polarization:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m matrix_double(u)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/perceval/components/unitary_components.py:124\u001b[0m, in \u001b[0;36mBS._compute_unitary\u001b[0;34m(self, assign, use_symbolic)\u001b[0m\n\u001b[1;32m    121\u001b[0m     u11_mul \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mcos(phi_bl_br) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39mj\u001b[38;5;241m*\u001b[39mmath\u001b[38;5;241m.\u001b[39msin(phi_bl_br)\n\u001b[1;32m    123\u001b[0m umat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_matrix_template(use_symbolic)\n\u001b[0;32m--> 124\u001b[0m \u001b[43mumat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m u00_mul\u001b[38;5;241m*\u001b[39mcos_theta\n\u001b[1;32m    125\u001b[0m umat[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m u01_mul\u001b[38;5;241m*\u001b[39msin_theta\n\u001b[1;32m    126\u001b[0m umat[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m u11_mul\u001b[38;5;241m*\u001b[39mcos_theta\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/perceval/utils/matrix.py:277\u001b[0m, in \u001b[0;36mMatrixS.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k):\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/sympy/matrices/repmatrix.py:336\u001b[0m, in \u001b[0;36mRepMatrix.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_getitem_RepMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/sympy/matrices/repmatrix.py:978\u001b[0m, in \u001b[0;36m_getitem_RepMatrix\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    976\u001b[0m i, j \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetitem_sympy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mIndexError\u001b[39;00m):\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(i, Expr) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m i\u001b[38;5;241m.\u001b[39mis_number) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(j, Expr) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m j\u001b[38;5;241m.\u001b[39mis_number):\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/sympy/polys/matrices/domainmatrix.py:211\u001b[0m, in \u001b[0;36mDomainMatrix.getitem_sympy\u001b[0;34m(self, i, j)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetitem_sympy\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, j):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sympy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/sympy/polys/domains/expressionrawdomain.py:35\u001b[0m, in \u001b[0;36mExpressionRawDomain.to_sympy\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sympify(a)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_sympy\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert ``a`` to a SymPy object. \"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we only train the first conv and fc layer\n",
    "MODES = 12\n",
    "\n",
    "pretrained_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# change first conv1 to take 1 input channel as input\n",
    "pretrained_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "# change last FC layer to map to the 10 classes of MNIST\n",
    "pretrained_model.fc = nn.Sequential(nn.Linear(in_features=512, out_features=MODES, bias = True), nn.Linear(in_features=MODES, out_features=10, bias = True))\n",
    "\n",
    "model = pretrained_model.to(device)\n",
    "model.requires_grad = False\n",
    "model.conv1.requires_grad = True\n",
    "model.fc.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001)\n",
    "train_model(model,optimizer,5,train_loader,val_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T20:38:32.582302Z",
     "start_time": "2025-03-25T20:38:32.374903Z"
    }
   },
   "outputs": [],
   "source": [
    "MODES = 8\n",
    "hidden_dim = 128\n",
    "\n",
    "# change first conv1 to take 1 input channel as input\n",
    "pretrained_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "pretrained_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "\n",
    "# first, we consider we have as may modes as features\n",
    "encoding_circuit = pcvl.Circuit(MODES)\n",
    "for i in range(0,MODES,2):\n",
    "    encoding_circuit.add(i , pcvl.BS())\n",
    "for i in range(1,MODES-1,2):\n",
    "    encoding_circuit.add(i , pcvl.BS())\n",
    "for i in range(hidden_dim):\n",
    "    feat = pcvl.P(f\"feat-{i + 1}\")\n",
    "    encoding_circuit.add(i % MODES, pcvl.PS(feat))\n",
    "pcvl.pdisplay(encoding_circuit)\n",
    "trainable_circuit = pcvl.GenericInterferometer(MODES, lambda i: (pcvl.BS() #theta=pcvl.P(f\"bs_1_{i}\")\n",
    "                                                             .add(0, pcvl.PS(pcvl.P(f\"phase_train_1_{i}\")))\n",
    "                                                             .add(0, pcvl.BS()) #theta=pcvl.P(f\"bs_1_{i}\")\n",
    "                                                             .add(0, pcvl.PS(pcvl.P(f\"phase_train_2_{i}\")))\n",
    "                                                             )\n",
    "                                          )\n",
    "circuit = pcvl.Circuit(MODES)\n",
    "circuit.add(0, encoding_circuit, merge=True)\n",
    "circuit.add(0, trainable_circuit, merge=True)\n",
    "\n",
    "input_state = [(i + 1) % 2 for i in range(MODES)]\n",
    "photon_count = sum(input_state)\n",
    "qlayer = QuantumLayer(\n",
    "            input_size=hidden_dim,\n",
    "            output_size=10, # but we do not use it\n",
    "            circuit = circuit,\n",
    "            input_state = input_state,\n",
    "            trainable_parameters= [p.name for p in circuit.get_parameters() if not p.name.startswith(\"feat\")],\n",
    "            output_mapping_strategy=OutputMappingStrategy.LINEAR)\n",
    "\n",
    "# change last FC layer to QLayer + linear layer\n",
    "pretrained_model.fc = nn.Sequential(nn.Linear(in_features=512, out_features=hidden_dim, bias=True),\n",
    "                                    qlayer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-25T20:38:33.139822Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [1:25:17<00:00,  8.53s/it]\n",
      "100%|██████████| 60/60 [08:15<00:00,  8.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Loss: 2.3043, Val Loss: 2.3119, Train Acc: 0.1110, Val Acc: 0.1050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [1:23:02<00:00,  8.30s/it]\n",
      "100%|██████████| 60/60 [08:14<00:00,  8.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: Train Loss: 2.3040, Val Loss: 2.3029, Train Acc: 0.1093, Val Acc: 0.1183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 55/600 [08:33<1:24:52,  9.34s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#model.requires_grad = False\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#model.conv1.requires_grad = True\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#model.fc.requires_grad = True\u001b[39;00m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, num_epochs, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m train_loss_epoch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m train_acc_epoch \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[1;32m     10\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     11\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/utils.py:103\u001b[0m, in \u001b[0;36mMNIST_quantum.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m     img_square \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img_square)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# print(img_square.shape)\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m img_square \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquanv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_square\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m img_square \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munflatten(torch\u001b[38;5;241m.\u001b[39mtensor(img_float),\u001b[38;5;241m0\u001b[39m,(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m))\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img_square, label\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/utils.py:71\u001b[0m, in \u001b[0;36mMNIST_quantum.quanv\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# Process a squared 2x2 region of the image with a quantum circuit\u001b[39;00m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;66;03m# print(image.shape)\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m         q_results \u001b[38;5;241m=\u001b[39m \u001b[43mqlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m# Assign expectation values to different channels of the output pixel (j/2, k/2)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pqnn_model.py:204\u001b[0m, in \u001b[0;36mQuantumLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    Forward pass through the quantum layer.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Output tensor [output_size] or [batch_size, output_size]\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     quantum_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_quantum_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_mapping(quantum_output)\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pqnn_model.py:190\u001b[0m, in \u001b[0;36mQuantumLayer.get_quantum_output\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_names):\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;66;03m# Scale input parameters by 2π for full phase shifter range\u001b[39;00m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit_parameter_map[name] \u001b[38;5;241m=\u001b[39m x[:, idx] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mpi\n\u001b[0;32m--> 190\u001b[0m unitaries \u001b[38;5;241m=\u001b[39m \u001b[43mpcvl_circuit_to_pytorch_unitary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircuit_parameter_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m _, distribution \u001b[38;5;241m=\u001b[39m pytorch_slos_output_distribution(unitaries, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_map_func)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m distribution\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pcvl2torch.py:214\u001b[0m, in \u001b[0;36mpcvl_circuit_to_pytorch_unitary\u001b[0;34m(circuit, circuit_parameters)\u001b[0m\n\u001b[1;32m    211\u001b[0m cU \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mcompute_unitary(use_symbolic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Convert to PyTorch, returns a batch of torch unitaries\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m cU_torch \u001b[38;5;241m=\u001b[39m \u001b[43msympy2torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Handle components that don't span all modes\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m!=\u001b[39m circuit\u001b[38;5;241m.\u001b[39mm:\n",
      "File \u001b[0;32m~/Downloads/Percival-Round-2/pcvl2torch.py:130\u001b[0m, in \u001b[0;36msympy2torch\u001b[0;34m(sympy_object, map_params, batch_size)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sympy_object\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sympy_object\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m--> 130\u001b[0m             t_object[:, i, j] \u001b[38;5;241m=\u001b[39m sympy2torch(sympy_object[i, j], map_params, batch_size)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Handle symbolic parameters: return the corresponding tensor from map_params\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sympy_object, sp\u001b[38;5;241m.\u001b[39mSymbol):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = pretrained_model.to(device)\n",
    "\n",
    "#model.requires_grad = False\n",
    "#model.conv1.requires_grad = True\n",
    "#model.fc.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.01)\n",
    "train_model(model,optimizer,5,train_loader,val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end session if needed\n",
    "if session is not None:\n",
    "    session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
